














!pip install scikit-learn-intelex


!pip install scikit-image


import glob
import os
import tarfile
import cv2
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import requests
import seaborn as sns
from skimage.feature import hog
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC

# Estas librerías son para optimizar la librería de sklearnex cuando 
# from sklearnex import patch_sklearn 
# patch_sklearn()








def download_and_extract_dataset(url: str, target_dir: str):
    """
    Descarga y extrae un conjunto de datos en formato tar.gz desde una URL en el directorio especificado.

    Args:
        url (str): La URL de donde se descargará el conjunto de datos.
        target_dir (str): El directorio donde se guardará el conjunto de datos.
    """
    # Revisar que el directorio objetivo existe
    os.makedirs(target_dir, exist_ok=True)

    # Establece la ruta donde se guardará el conjunto de datos
    filename = url.split("/")[-1]
    file_path = os.path.join(target_dir, filename)

    # Verifica si el archivo ya existe
    if not os.path.exists(file_path):
        print("Descargando el conjunto de datos...")
        # Envía una solicitud GET a la URL
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            # Guarda el archivo en fragmentos para evitar usar demasiada memoria
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            print("Descarga completa. Extrayendo archivos...")
            # Extrae el archivo tar.gz
            with tarfile.open(file_path, "r:gz") as tar:
                tar.extractall(path=target_dir)
            print("Extracción completa.")
        else:
            print("Fallo al descargar el archivo. Código de estado:", response.status_code)
    else:
        print("El conjunto de datos ya existe.")





dataset_url = "http://www.lookingatpeople.com/data/Daimler/pami06-munder-gavrila/DC-ped-dataset_base.tar.gz"
dataset_target_dir = "data/pedestrians"

download_and_extract_dataset(dataset_url, dataset_target_dir)











def get_data_files(sub_dir: str, sub_dirs: str, file_extension: str='pgm') -> list:
    data = []
    for sub_dir in sub_dirs:
        data_files = glob.glob(f"{dataset_target_dir}/{sub_dir}/*.{file_extension}")
        data.extend(data_files)
    return data








train_pedestrians_sub_dirs = ["1/ped_examples", "2/ped_examples", "3/ped_examples"]
train_pedestrians = get_data_files(dataset_target_dir, train_pedestrians_sub_dirs)
len(train_pedestrians)





train_non_pedestrians_sub_dirs = ["1/non-ped_examples", "2/non-ped_examples", "3/non-ped_examples"]
train_non_pedestrians = get_data_files(dataset_target_dir, train_non_pedestrians_sub_dirs)
len(train_non_pedestrians)





test_pedestrians_sub_dirs = ["T1/ped_examples", "T2/ped_examples"]
test_pedestrians = get_data_files(dataset_target_dir, test_pedestrians_sub_dirs)
len(test_pedestrians)


test_non_pedestrians_sub_dirs = ["T1/non-ped_examples", "T2/non-ped_examples"]
test_non_pedestrians = get_data_files(dataset_target_dir, test_non_pedestrians_sub_dirs)
len(test_non_pedestrians)








img_pedestrian = cv2.imread(train_pedestrians[170], cv2.COLOR_BGR2GRAY)
plt.imshow(img_pedestrian, cmap='gray')
plt.show()





print(img_pedestrian.shape)





features, hog_image = hog(img_pedestrian,
                          orientations=10,
                          pixels_per_cell=(6, 6),
                          cells_per_block=(2, 2),
                          transform_sqrt= False,
                          visualize=True,
                          feature_vector=True)


print(features.shape)


plt.imshow(hog_image, cmap='gray')
plt.show()








def extract_hog_features(img_data_list: list) -> list:
    hog_data = list()
    for img_data in img_data_list:
        img = cv2.imread(img_data, cv2.COLOR_BGR2GRAY)
        img_hog_feature, img_hog_image = hog(img,
                                             orientations=10,
                                             pixels_per_cell=(6, 6),
                                             cells_per_block=(2, 2),
                                             transform_sqrt= False,
                                             visualize=True,
                                             feature_vector=True)
        hog_data.append(img_hog_feature)

    return hog_data





X_train_pedestrians = np.vstack(extract_hog_features(train_pedestrians)).astype(np.float32)
y_train_pedestrians = np.ones(len(X_train_pedestrians))

X_train_non_pedestrians = np.vstack(extract_hog_features(train_non_pedestrians)).astype(np.float32)
y_train_non_pedestrians = np.zeros(len(X_train_non_pedestrians))

X_train = np.vstack((X_train_pedestrians, X_train_non_pedestrians))
y_train = np.hstack((y_train_pedestrians, y_train_non_pedestrians))

# Poner datos de forma aleatoria
random_state = np.random.RandomState(seed=42)
indices = random_state.permutation(len(X_train))
X_train_shuffled = X_train[indices]
y_train_shuffled = y_train[indices]





del X_train
del y_train
del train_pedestrians
del train_non_pedestrians





X_test_pedestrians = np.vstack(extract_hog_features(test_pedestrians)).astype(np.float32)
y_test_pedestrians = np.ones(len(X_test_pedestrians))

X_test_non_pedestrians = np.vstack(extract_hog_features(test_non_pedestrians)).astype(np.float32)
y_test_non_pedestrians = np.zeros(len(X_test_non_pedestrians))

X_test = np.vstack((X_test_pedestrians, X_test_non_pedestrians))
y_test = np.hstack((y_test_pedestrians, y_test_non_pedestrians))

# Poner datos de forma aleatoria
indices = random_state.permutation(len(X_test))
X_test_shuffled = X_test[indices]
y_test_shuffled = y_test[indices]





del X_test
del y_test
del test_pedestrians
del test_non_pedestrians








svc_model = SVC()
svc_model.fit(X_train_shuffled, y_train_shuffled)





y_pred = svc_model.predict(X_test_shuffled)


# Guardar el modelo resultante
joblib.dump(svc_model, 'svc_model.pkl')








cm = confusion_matrix(y_test_shuffled, y_pred, labels=[0, 1])
sns.heatmap(cm,  annot=True, fmt='g', cmap='Purples', cbar=False)
plt.show()





print(cm)


print(classification_report(y_test_shuffled, y_pred))











param_grid = {'C': [1, 10, 100, 1000], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1], 'kernel': ['rbf']}





grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=4)
grid_search.fit(X_train_shuffled, y_train_shuffled)





grid_search.best_params_


grid_search.best_estimator_





grid_predictions = grid_search.predict(X_test_shuffled)





cm = confusion_matrix(y_test_shuffled, grid_predictions)
sns.heatmap(cm, annot=True, fmt='g', cbar=False)
plt.show()


cm





print(classification_report(y_test_shuffled, grid_predictions))





import joblib

# Guardar el modelo resultante
joblib.dump(grid_search, 'modelo_entrenado.pkl')

# Realizar predicciones con el modelo y guardarlas
grid_predictions = grid_search.predict(X_test_shuffled)
joblib.dump(grid_predictions, 'predicciones.pkl')





# Cargar el modelo
modelo_cargado = joblib.load('modelo_entrenado.pkl')

# Cargar las predicciones
predicciones_cargadas = joblib.load('predicciones.pkl')
